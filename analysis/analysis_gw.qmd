---
title: "Introduction Exploratory Data Analysis"
author: George Whittington
date: today
date-format: long
---

# Set Up

```{python}
#| label: libraries
#| output: false

import polars as pl
import polars.selectors as cs

import numpy as np

import altair as alt
import great_tables

from pyhere import here
```

# Source Apportionment

Source apportionment is a "collection of techniques to provide information
regarding how much a source (usually a generalized category) contributes to the
overall pollutant concentration at receptor (usually a monitoring site)"
(Rizzo, 2010).

"Source apportionment is the process of statistically estimating the relative
contributions from a set of sources to the observed sample data" (Smith, 2025).

In our case, we will use source apportionment as a method of unsupervised
learning applied to air quality data. By analyzing the chemical composition of
each observation, we will use Non-Negative/Positive Matrix Factorization
(NMF/PMF) to extract latent factors from the data. We can then interpret these
factors to identify specific source profiles (the chemical makeup of the source)
and calculate their corresponding source contributions (the amount of pollution
from that source) over time.

Statistically, this approach functions as a specialized form of Factor Analysis.
While traditional Factor Analysis allows for negative values in its mathematical
solution, our context requires a strict non-negativity constraint because
physical matter (pollution) cannot exist in negative quantities. Therefore, we
treat the unknown pollution sources as latent factors (hidden variables that
drive the observed variance in the data) and solve for them using algorithms
that enforce positive values.

# Datasets

For this project, the EPA has provided six data files:

- Dataset-Baltimore_con.csv
- Dataset-Baltimore_unc.csv

- Dataset-BatonRouge-con.csv
- Dataset-BatonRouge-unc.csv

- Dataset-StLouis-con.csv
- Dataset-StLouis-unc.csv

Where the *_con files represent the readings from the detection device and the
*_unc files represent the uncertainty of those readings, or the standard
deviation of the readings. This comes into play where we can see how reliable
the readings of each chemical were for that day. A flat 10% uncertainty could
just be a baseline for that given chemical. But given an uncertainty that is
higher than the observed value, that might indicate that the observation fell
out of the range of detection for the measurement device. 

## Baltimore

Read in both concentration and uncertainty files

```{python}
#| label: load-baltimore

# read in each as it
baltimore_concentration = pl.read_csv(here("data/Dataset-Baltimore_con.csv"))
baltimore_uncertainty = pl.read_csv(here("data/Dataset-Baltimore_unc.csv"))

# convert the Date column to a proper date
baltimore_concentration_log = baltimore_concentration.with_columns(
    pl.col("Date").str.to_date(r"%m/%d/%Y"),
    cs.numeric().log1p()
)
baltimore_uncertainty_log = baltimore_uncertainty.with_columns(
    pl.col("Date").str.to_date(r"%m/%d/%Y"),
    cs.numeric().log1p()
)
```

### Concentration

```{python}
#| label: first-glimpse-baltimore-concentration

baltimore_concentration_log.with_columns(
    cs.numeric().round(5)
).glimpse()
```

Here we can see the data type and a top glimpse into the value of each of the
columns. Here can see there is one column of dates, and then the concentration
of each of the chemical/elements give as a double. 

```{python}
#| label: tbl-head-tail-baltimore-concentration
#| tbl-cap: First and Last 5 Rows

(
    pl.concat([
        baltimore_concentration_log.head(5),
        baltimore_concentration_log.tail(5)
    ])
    .style
    .fmt_number(cs.numeric(), decimals=5)
)
```

### Uncertainty

## BatonRouge

Read in both concentration and uncertainty files

```{python}
#| label: load-baton-rouge

# read in each as it
baton_rouge_concentration = pl.read_csv(here("data/Dataset-BatonRouge-con.csv"))
baton_rouge_uncertainty = pl.read_csv(here("data/Dataset-BatonRouge-unc.csv"))

# convert the Date column to a proper date
baton_rouge_concentration_log = baton_rouge_concentration.with_columns(
    pl.col("Date").str.to_datetime(r"%m/%d/%Y %H:%M"),
    cs.numeric().log1p()
)
baton_rouge_uncertainty_log = baton_rouge_uncertainty.with_columns(
    pl.col("Date").str.to_datetime(r"%m/%d/%Y %H:%M"),
    cs.numeric().log1p()
)
```

### Concentration

```{python}
#| label: first-glimpse-baton-rouge-concentration

baton_rouge_concentration_log.with_columns(
    cs.numeric().round(5)
).glimpse()
```

```{python}
#| label: tbl-head-tail-baton-rouge-concentration
#| tbl-cap: First and Last 5 Rows

(
    pl.concat([
        baton_rouge_concentration_log.head(5),
        baton_rouge_concentration_log.tail(5)
    ])
    .style
    .fmt_datetime(
        columns="Date",
        date_style="iso",
        time_style="iso-short"
    )
    .fmt_number(cs.numeric(), decimals=5)
)
```

### Uncertainty

## StLouis

Read in both concentration and uncertainty files

```{python}
#| label: load-st-louis

# read in each as it
st_louis_concentration = pl.read_csv(here("data/Dataset-StLouis-con.csv"))
st_louis_uncertainty = pl.read_csv(here("data/Dataset-StLouis-unc.csv"))

# convert the Date column to a proper date
st_louis_concentration_log = st_louis_concentration .with_columns(
    pl.col("Date").str.to_datetime(r"%m/%d/%Y %H:%M"),
    cs.numeric().log1p()
)
st_louis_uncertainty_log = st_louis_uncertainty.with_columns(
    pl.col("Date").str.to_datetime(r"%m/%d/%Y %H:%M"),
    cs.numeric().log1p()
)
```

### Concentration

```{python}
#| label: first-glimpse-st-louis-concentration

st_louis_concentration_log.with_columns(
    cs.numeric().round(5)
).glimpse()
```

```{python}
#| label: tbl-head-tail-st-louis-concentration
#| tbl-cap: First and Last 5 Rows

(
    pl.concat([
        st_louis_concentration_log.head(5),
        st_louis_concentration_log.tail(5)
    ])
    .style
    .fmt_datetime(
        columns="Date",
        date_style="iso",
        time_style="iso-short"
    )
    .fmt_number(cs.numeric(), decimals=5)
)
```

### Uncertainty

# Creation of Long Datasets

Function for reference creating the long datasets

```{python}
#| label: function-write-long
#| eval: false

def write_long_data(
    data_con: pl.DataFrame,
    data_unc: pl.DataFrame,
    file_name: str
):
    """
    Convert EPA Data to Long Format with uncertainty and write to csv
    and parquet (to optionally preserve data type information)

    Parameters
    ----------
    data_con : pl.DataFrame
        A DataFrame containing the concentration data
    data_unc : pl.DataFrame
        A DataFrame uncertainty the concentration data
    file_name : str
        A file name to save data as
    """

    # keep the Date column as it and move the names and values of each
    # species to a row
    data_con_long = data_con.unpivot(
        on=cs.numeric(),
        index="Date",
        variable_name="Species",
         value_name="Concentration"
    )

    # keep the Date column as it and move the names and values of each
    # species to a row
    data_unc_long = data_unc.unpivot(
        on=cs.numeric(),
        index="Date",
        variable_name="Species",
         value_name="Uncertainty"
    )

    # join them together and add uncertainty percentage
    combined = data_con_long.join(
        data_unc_long,
        on=["Date", "Species"],
    )

    # St Louis has 0 values in concentration
    combined = combined.with_columns(
        pl.col("Concentration").replace(0, None)
        .alias("Concentration")
    )

    combined = combined.with_columns(
        (pl.col("Uncertainty") / pl.col("Concentration"))
        .alias("Uncertainty_Ratio")
    )

    # write combined data to file
    combined.write_csv(here("data/long", file_name + ".csv"))
    combined.write_parquet(here("data/long", file_name + ".parquet"))
```

```{python}
#| label: write-data-long
#| eval: false

write_long_data(
    baltimore_concentration,
    baltimore_uncertainty,
    "baltimore_long."
)
write_long_data(
    baton_rouge_concentration,
    baton_rouge_uncertainty,
    "baton_rouge_long"
)
write_long_data(
    st_louis_concentration,
    st_louis_uncertainty,
    "st_louis_long"
)
```

# Links

## Useful Reference for modeling later

<a href="https://cloud.r-project.org/web/packages/pcpr/vignettes/pcp-applied.html" target="_blank" rel="noopener noreferrer">R code example doing Non-Negative Matrix Factorization</a>

<a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html" target="_blank" rel="noopener noreferrer">Python code example doing Non-Negative Matrix Factorization</a>

## References

<a href="https://www.nrcs.usda.gov/sites/default/files/2022-10/4-Source-Apportionment.pdf" target="_blank" rel="noopener noreferrer">Powerpoint also on elc. This is Rizzo</a>
